return("Mixed Precipitation")
} else {
return("Other")
}
}
df_new$Weather_Condition<- sapply(df_new$Weather_Condition, map_weather_conditions)
df_new$Weather_Condition <- as.factor(df_new$Weather_Condition)
# Set the desired number of samples per class
samples_per_class <- 10000
# Undersample majority class (Severity 2)
undersampled_df2 <- df_new %>%
filter(Severity == "2") %>%
sample_n(samples_per_class)
# Oversample minority classes (Severity 1, 3, and 4)
oversampled_df1 <- df_new %>%
filter(Severity == "1") %>%
sample_n(samples_per_class, replace = TRUE)
oversampled_df3 <- df_new %>%
filter(Severity == "3") %>%
sample_n(samples_per_class, replace = TRUE)
oversampled_df4 <- df_new %>%
filter(Severity == "4") %>%
sample_n(samples_per_class, replace = TRUE)
# Combine the undersampled and oversampled data
balanced_df <- bind_rows(oversampled_df1, undersampled_df2, oversampled_df3, oversampled_df4)
# Shuffle the balanced data
balanced_df <- balanced_df[sample(nrow(balanced_df)),]
# Define the proportion of training, testing, and validation sets
train_prop <- 0.7
test_prop <- 0.2
valid_prop <- 0.1
# Calculate the number of rows for each set
train_n <- round(nrow(balanced_df) * train_prop)
test_n <- round(nrow(balanced_df) * test_prop)
# Split the balanced data into training, testing, and validation sets (same code as before)
train_data <- balanced_df[1:train_n, ]
test_data <- balanced_df[(train_n + 1):(train_n + test_n), ]
valid_data <- balanced_df[(train_n + test_n + 1):nrow(balanced_df), ]
# Set the seed for reproducibility
set.seed(123)
# Combine the training and validation sets
train_valid_data <- bind_rows(train_data, valid_data)
library(Boruta)
sample_size <- 1000
train_valid_sample <- train_valid_data %>% sample_n(sample_size)
X_sample <- dplyr::select(train_valid_sample, -Severity) %>% model.matrix(~ . - 1, .)
y_sample <- train_valid_sample$Severity
# Perform feature selection using the Boruta algorithm
boruta_results <- Boruta(Severity ~ ., data = train_valid_sample, doTrace = 0)
# Get the optimal subset of features
optimal_features <- getSelectedAttributes(boruta_results, withTentative = TRUE)
optimal_features <- c(optimal_features, "Weather_Condition")
train_valid_optimal <- train_valid_data %>%
dplyr::select(Severity, optimal_features)
# Install and load required packages
# Define the training control for cross-validation
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = multiClassSummary, allowParallel = FALSE)
# Modify the class levels of the Severity variable
train_valid_optimal$Severity <- factor(train_valid_optimal$Severity, labels = paste0("S", levels(train_valid_optimal$Severity)))
write.csv(train_valid_optimal, "train_valid_optimal.csv", row.names = FALSE)
load("/Users/yanweitong/512-project-group-08/code/data_analysis/train_control.RData")
View(train_control)
load("/Users/yanweitong/512-project-group-08/code/data_analysis/train_valid_optimal.RData")
View(train_valid_optimal)
library(tidyverse)
library(dplyr)
library(plotly)
library(ggplot2)
library(caret)
library(e1071)
library(randomForest)
library(nnet)
library(gbm)
library(MASS)
library(rpart)
library(ROCR)
library(MLmetrics)
# Re-run model with best hyperparameters...
set.seed(123)
bfit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "treebag",
minbucket = 2,
trControl = train_control,
importance = TRUE,
maxdepth = 5,
minsplit = 5,
)
bfit$results$Mean_Balanced_Accuracy
bfit$results$AUC
load("/Users/yanweitong/512-project-group-08/code/data_analysis/test_data.RData")
rffit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "rf",
trControl = train_control,
importance = TRUE,
.mtry= 33,
ntree = 200,
nodesize = 1
)
mean(rffit$results$Mean_Balanced_Accuracy)
mean(rffit$results$AUC)
best.fit.pred1 <-  predict(rffit, newdata = test_data)
levels(best.fit.pred1) = c(1,2,3,4)
cfm_rf <- tibble("Actual" = test_data$Severity,
"Predicted" = best.fit.pred1)
cfm_rf <- table(cfm_rf)
cfm_rf <- as_tibble(cfm_rf)
plot_confusion_matrix(cfm_rf,
target_col = "Actual",
prediction_col = "Predicted",
counts_col = "n")
View(test_data)
test_data = test_data[,-X]
test_data = test_data[,-"X"]
test_data = test_data[,-1]
View(test_data)
View(train_valid_optimal)
train_valid_optimal=train_valid_optimal[,-1]
# Re-run model with best hyperparameters...
set.seed(123)
bfit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "treebag",
minbucket = 2,
trControl = train_control,
importance = TRUE,
maxdepth = 5,
minsplit = 5,
)
View(train_valid_optimal)
load("/Users/yanweitong/512-project-group-08/code/data_analysis/train_control.RData")
load("/Users/yanweitong/512-project-group-08/code/data_analysis/train_valid_optimal.RData")
load("/Users/yanweitong/512-project-group-08/code/data_analysis/test_data.RData")
train_valid_optimal = train_valid_optimal%>%dplyr::select(-X)
test_data = test_data%>%dplyr::select(-X)
View(test_data)
View(train_valid_optimal)
# Re-run model with best hyperparameters...
set.seed(123)
bfit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "treebag",
minbucket = 2,
trControl = train_control,
importance = TRUE,
maxdepth = 5,
minsplit = 5,
)
bfit$results$Mean_Balanced_Accuracy
bfit$results$AUC
best.fit.pred <-  predict(bfit, newdata = test_data)
levels(best.fit.pred) = c(1,2,3,4)
cfm_bag <- tibble("Actual" = test_data$Severity,
"Predicted" = best.fit.pred)
cfm_bag <- table(cfm_bag)
cfm_bag <- as_tibble(cfm_bag)
plot_confusion_matrix(cfm_bag,
target_col = "Actual",
prediction_col = "Predicted",
counts_col = "n")
View(bfit)
rffit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "rf",
trControl = train_control,
importance = TRUE,
.mtry= 33,
ntree = 200,
nodesize = 1
)
mean(rffit$results$Mean_Balanced_Accuracy)
mean(rffit$results$AUC)
treebag_prob <- predict(bfit, train_valid_optimal, type = "prob")
rf_prob <- predict(rffit, train_valid_optimal, type = "prob")
library(pROC)
treebag_roc <- roc(response = train_valid_optimal$Severity, predictor = treebag_prob[, 2])
rf_roc <- roc(response = train_valid_optimal$Severity, predictor = rf_prob[, 2])
treebag_auc <- auc(treebag_roc)
rf_auc <- auc(rf_roc)
library(ggplot2)
roc_data <- data.frame(
FPR = c(treebag_roc$specificities - 1, rf_roc$specificities - 1),
TPR = c(treebag_roc$sensitivities, rf_roc$sensitivities),
Model = rep(c("Treebag", "Random Forest"), each = length(treebag_roc$sensitivities))
)
View(rf_roc)
View(rf_roc)
View(treebag_roc)
treebag_roc[["sensitivities"]]
treebag_roc[["specificities"]]
dim(rf_roc[["sensitivities"]])
rf_roc[["specificities"]]
treebag_roc_data <- data.frame(
FPR = treebag_roc$specificities - 1,
TPR = treebag_roc$sensitivities,
Model = "Treebag"
)
rf_roc_data <- data.frame(
FPR = rf_roc$specificities - 1,
TPR = rf_roc$sensitivities,
Model = "Random Forest"
)
roc_data <- rbind(treebag_roc_data, rf_roc_data)
View(roc_data)
library(ggplot2)
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(size = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
library(ggplot2)
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
library(ggplot2)
ggplot(roc_data, aes(x = TPR, y = FPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
library(ggplot2)
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
treebag_roc[["specificities"]]
print(roc_data)
View(train_valid_optimal)
# Remove the 'S' from the Severity column
train_valid_optimal$Severity <- gsub("S", "", train_valid_optimal$Severity)
View(train_valid_optimal)
# Re-run model with best hyperparameters...
set.seed(123)
bfit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "treebag",
minbucket = 2,
trControl = train_control,
importance = TRUE,
maxdepth = 5,
minsplit = 5,
)
View(rf_roc)
load("/Users/yanweitong/512-project-group-08/code/data_analysis/train_control.RData")
load("/Users/yanweitong/512-project-group-08/code/data_analysis/train_valid_optimal.RData")
load("/Users/yanweitong/512-project-group-08/code/data_analysis/test_data.RData")
train_valid_optimal = train_valid_optimal%>%dplyr::select(-X)
test_data = test_data%>%dplyr::select(-X)
test_data$Severity <- factor(test_data$Severity, labels = paste0("S", levels(test_data$Severity)))
View(test_data)
# Re-run model with best hyperparameters...
set.seed(123)
bfit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "treebag",
minbucket = 2,
trControl = train_control,
importance = TRUE,
maxdepth = 5,
minsplit = 5,
)
rffit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "rf",
trControl = train_control,
importance = TRUE,
.mtry= 33,
ntree = 200,
nodesize = 1
)
treebag_prob <- predict(bfit, test_data, type = "prob")
rf_prob <- predict(rffit, test_data, type = "prob")
library(pROC)
treebag_roc <- roc(response = test_data$Severity, predictor = treebag_prob[, 2])
rf_roc <- roc(response = test_data$Severity, predictor = rf_prob[, 2])
treebag_auc <- auc(treebag_roc)
rf_auc <- auc(rf_roc)
treebag_roc_data <- data.frame(
FPR = treebag_roc$specificities - 1,
TPR = treebag_roc$sensitivities,
Model = "Treebag"
)
rf_roc_data <- data.frame(
FPR = rf_roc$specificities - 1,
TPR = rf_roc$sensitivities,
Model = "Random Forest"
)
roc_data <- rbind(treebag_roc_data, rf_roc_data)
library(ggplot2)
ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
View(roc_data)
library(ggplot2)
roc_plot= ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
ggsave(filename = "roc.png", plot = roc_plot, width = 10, height = 6, units = "in", dpi = 300)
library(ggplot2)
roc_plot= ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
roc_plot
treebag_roc_data <- data.frame(
FPR = 1- treebag_roc$specificities,
TPR = treebag_roc$sensitivities,
Model = "Treebag"
)
rf_roc_data <- data.frame(
FPR = 1 - rf_roc$specificities,
TPR = rf_roc$sensitivities,
Model = "Random Forest"
)
roc_data <- rbind(treebag_roc_data, rf_roc_data)
library(ggplot2)
roc_plot= ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag and Random Forest Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
roc_plot
ggsave(filename = "roc.png", plot = roc_plot, width = 10, height = 6, units = "in", dpi = 300)
# Save test_data and train_valid_optimal as .RData file
save(test_data, train_valid_optimal, file = "my_data.RData")
View(train_control)
print(train_control)
library(caret)
# Fine-tune and fit LDA model
lda_fit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "lda",
trControl = train_control,
preProcess = c("center", "scale")
)
# Fine-tune and fit Logistic Regression model
logistic_fit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "glm",
family = "binomial",
trControl = train_control,
preProcess = c("center", "scale")
)
library(caret)
# Fine-tune and fit LDA model
lda_grid <- expand.grid(shrinkage = seq(0, 1, 0.1))
lda_fit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "lda2",
trControl = train_control,
preProcess = c("center", "scale"),
tuneGrid = lda_grid
)
# Load required libraries
library(MASS)
# Define a range of shrinkage values
shrinkage_values <- seq(0, 1, 0.1)
# Initialize a list to store LDA models and their performance
lda_models <- list()
# Loop through each shrinkage value
for (i in 1:length(shrinkage_values)) {
shrinkage <- shrinkage_values[i]
# Train the LDA model with the current shrinkage value
lda_model <- lda(Severity ~ .,
data = train_valid_optimal,
prior = rep(1/length(unique(train_valid_optimal$Severity)), length(unique(train_valid_optimal$Severity))),
method = "moment",
CV = TRUE,
nu = shrinkage)
# Store the LDA model and its performance in the list
lda_models[[i]] <- list("model" = lda_model, "accuracy" = mean(lda_model$class == train_valid_optimal$Severity))
}
# Find the best LDA model
best_lda_model_index <- which.max(sapply(lda_models, function(x) x$accuracy))
best_lda_model <- lda_models[[best_lda_model_index]]$model
best_shrinkage_value <- shrinkage_values[best_lda_model_index]
# Fine-tune and fit Logistic Regression model
logistic_grid <- expand.grid(alpha = c(0, 1), # L1 penalty
lambda = seq(0, 1, 0.1)) # L2 penalty
logistic_fit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "glmnet",
family = "binomial",
trControl = train_control,
preProcess = c("center", "scale"),
tuneGrid = logistic_grid
)
# Fine-tune and fit Logistic Regression model
logistic_grid <- expand.grid(alpha = c(0, 1), # L1 penalty
lambda = seq(0, 1, 0.1)) # L2 penalty
logistic_fit <- train(
Severity ~ .,
data = train_valid_optimal,
method = "glmnet",
family = "multinomial",
trControl = train_control,
preProcess = c("center", "scale"),
tuneGrid = logistic_grid
)
# Best Logistic Regression model parameters
best_logistic <- logistic_fit$bestTune
View(best_logistic)
# Predict probabilities for LDA and Logistic Regression models on test_data
lda_prob <- predict(best_lda_model, test_data, type = "posterior")
# Predict probabilities for LDA and Logistic Regression models on test_data
lda_pred <- MASS::predict(best_lda_model, test_data)
# Predict probabilities for LDA and Logistic Regression models on test_data
lda_pred <- MASS::predict.lda(best_lda_model, test_data)
View(best_lda_model)
# Predict probabilities for LDA and Logistic Regression models on test_data
lda_pred <- predict(best_lda_model, test_data)
class(best_lda_model)
View(lda_model)
lda_model[["posterior"]]
View(lda_models)
# Predict probabilities for LDA and Logistic Regression models on test_data
# Extract the LDA model object from the list
best_lda_model_object <- best_lda_model$model
# Predict probabilities for LDA and Logistic Regression models on test_data
lda_prob <- predict(best_lda_model_object, test_data, type = "posterior")
# Retrain the LDA model with the best shrinkage value without using cross-validation
best_lda_model_no_cv <- lda(Severity ~ .,
data = train_valid_optimal,
prior = rep(1/length(unique(train_valid_optimal$Severity)), length(unique(train_valid_optimal$Severity))),
method = "moment",
nu = best_shrinkage_value)
# Predict probabilities for LDA model on test_data
lda_prob <- predict(best_lda_model_no_cv, test_data, type = "posterior")
# Predict probabilities for LDA and Logistic Regression models on test_data
# Predict probabilities for LDA model on test_data
lda_prob <- predict(best_lda_model_no_cv, test_data, type = "posterior")
logistic_prob <- predict(logistic_fit, newdata = test_data, type = "prob")
# Calculate ROC and AUC for LDA and Logistic Regression models
lda_roc <- roc(response = test_data$Severity, predictor = lda_prob[, 2])
View(lda_prob)
print(lda_prob)
# Extract the posterior probabilities for the LDA model
lda_posterior_prob <- lda_prob$posterior
# Calculate ROC and AUC for LDA and Logistic Regression models
lda_roc <- roc(response = test_data$Severity, predictor = lda_posterior_prob[, 2])
# Calculate ROC and AUC for LDA and Logistic Regression models
logistic_roc <- roc(response = test_data$Severity, predictor = logistic_prob[, 2])
lda_auc <- auc(lda_roc)
logistic_auc <- auc(logistic_roc)
# Combine ROC data
lda_roc_data <- data.frame(
FPR = 1 - lda_roc$specificities,
TPR = lda_roc$sensitivities,
Model = "LDA"
)
logistic_roc_data <- data.frame(
FPR = 1 - logistic_roc$specificities,
TPR = logistic_roc$sensitivities,
Model = "Logistic Regression"
)
library(ggplot2)
roc_data <- rbind(treebag_roc_data, rf_roc_data, lda_roc_data, logistic_roc_data)
roc_plot <- ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
geom_line(linewidth = 1) +
geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
labs(title = "ROC Curves for Treebag, Random Forest, LDA, and Logistic Regression Models",
x = "False Positive Rate (1 - Specificity)",
y = "True Positive Rate (Sensitivity)") +
theme_minimal() +
theme(legend.title = element_blank())
roc_plot
ggsave(filename = "roc.png", plot = roc_plot, width = 10, height = 6, units = "in", dpi = 300)
