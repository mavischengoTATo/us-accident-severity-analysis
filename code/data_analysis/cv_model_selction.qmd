---
title: "Cross-Validation model comprasion"
author:
  - name: Yichen Guo
    affiliations:
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    embed-resources: true
    theme: cosmo
    code-fold: true
    code-copy: true
    code-line-numbers: true
    number-sections: true
    highlight-style: github
reference-location: margin
---

```{r ,echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dplyr)
library(plotly)
library(ggplot2)
library(caret)
library(e1071)
library(randomForest)
library(nnet)
library(gbm)
library(MASS)
library(rpart)
library(ROCR)
library(MLmetrics)

```

```{r}
source = read.csv("../../data/clean_data/clean_data_2020.csv")
```


```{r}
df_new <- subset(source, select = -c(X, ID, Start_Time, End_Time, Start_Lat, Start_Lng, End_Lat, End_Lng, Street, City, County, Zipcode, Precipitation.in., Civil_Twilight, Nautical_Twilight, Astronomical_Twilight, Start_Hour, Start_Month, Start_Date))

# Specify the columns to convert to factors
cols <- c("Severity","Side","Amenity", "Bump", "Crossing", "Give_Way", "Junction","No_Exit","Railway","Roundabout","Station","Stop","Traffic_Calming","Traffic_Signal","Sunrise_Sunset")

# Convert the columns to factors
df_new[cols] <- lapply(df_new[cols], factor)

```


```{r}
# mapping the weather conditions so reduce the calcualtion load

map_weather_conditions <- function(condition) {
  if (grepl("Snow|Sleet|Freezing|Ice|Blowing Snow|Drifting Snow", condition)) {
    return("Snow/Ice")
  } else if (grepl("Rain|Drizzle|Shower|T-Storm", condition)) {
    return("Rain")
  } else if (grepl("Thunder|Hail", condition)) {
    return("Thunderstorm")
  } else if (grepl("Fog|Mist|Haze|Partial Fog|Shallow Fog|Patches of Fog", condition)) {
    return("Fog/Mist")
  } else if (grepl("Cloudy|Overcast|Fair", condition)) {
    return("Cloudy")
  } else if (grepl("Dust|Sand|Smoke|Widespread Dust|Blowing Dust", condition)) {
    return("Dust/Smoke")
  } else if (grepl("Wintry Mix|Squalls", condition)) {
    return("Mixed Precipitation")
  } else {
    return("Other")
  }
}

df_new$Weather_Condition<- sapply(df_new$Weather_Condition, map_weather_conditions)
df_new$Weather_Condition <- as.factor(df_new$Weather_Condition)
```


```{r}
# Set the desired number of samples per class
samples_per_class <- 1000

# Undersample majority class (Severity 2)
undersampled_df2 <- df_new %>%
  filter(Severity == "2") %>%
  sample_n(samples_per_class)

# Oversample minority classes (Severity 1, 3, and 4)
oversampled_df1 <- df_new %>%
  filter(Severity == "1") %>%
  sample_n(samples_per_class, replace = TRUE)

oversampled_df3 <- df_new %>%
  filter(Severity == "3") %>%
  sample_n(samples_per_class, replace = TRUE)

oversampled_df4 <- df_new %>%
  filter(Severity == "4") %>%
  sample_n(samples_per_class, replace = TRUE)

# Combine the undersampled and oversampled data
balanced_df <- bind_rows(oversampled_df1, undersampled_df2, oversampled_df3, oversampled_df4)

# Shuffle the balanced data
balanced_df <- balanced_df[sample(nrow(balanced_df)),]


# Define the proportion of training, testing, and validation sets
train_prop <- 0.7
test_prop <- 0.2
valid_prop <- 0.1

# Calculate the number of rows for each set
train_n <- round(nrow(balanced_df) * train_prop)
test_n <- round(nrow(balanced_df) * test_prop)

# Split the balanced data into training, testing, and validation sets (same code as before)
train_data <- balanced_df[1:train_n, ]
test_data <- balanced_df[(train_n + 1):(train_n + test_n), ]
valid_data <- balanced_df[(train_n + test_n + 1):nrow(balanced_df), ]

```

```{r}
# Save the training, testing, and validation sets as separate CSV files
write.csv(train_data, "train_data.csv", row.names = FALSE)
write.csv(test_data, "test_data.csv", row.names = FALSE)
write.csv(valid_data, "valid_data.csv", row.names = FALSE)

```


```{r}
# Set the seed for reproducibility
set.seed(123)

# Combine the training and validation sets
train_valid_data <- bind_rows(train_data, valid_data)

library(Boruta)

sample_size <- 1000
train_valid_sample <- train_valid_data %>% sample_n(sample_size)
X_sample <- dplyr::select(train_valid_sample, -Severity) %>% model.matrix(~ . - 1, .)
y_sample <- train_valid_sample$Severity

# Perform feature selection using the Boruta algorithm
boruta_results <- Boruta(Severity ~ ., data = train_valid_sample, doTrace = 0)

# Get the optimal subset of features
optimal_features <- getSelectedAttributes(boruta_results, withTentative = TRUE)

optimal_features <- c(optimal_features, "Weather_Condition")

train_valid_optimal <- train_valid_data %>%
  dplyr::select(Severity, optimal_features)
```

```{r}
# Install and load required packages

# Define the training control for cross-validation
train_control <- trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = multiClassSummary, allowParallel = FALSE)

# Modify the class levels of the Severity variable
train_valid_optimal$Severity <- factor(train_valid_optimal$Severity, labels = paste0("S", levels(train_valid_optimal$Severity)))
```


```{r}
# Bagging
bagging_model <- train(Severity ~ ., data = train_valid_optimal, method = "treebag", trControl = train_control)
bagging_results <- bagging_model$results
```


```{r}
# Naive Bayes
naive_bayes_model <- train(Severity ~ ., data = train_valid_optimal, method = "naive_bayes", trControl = train_control)
naive_bayes_results <- naive_bayes_model$results
```


```{r}
# LDA
lda_model <- train(Severity ~ ., data = train_valid_optimal, method = "lda", trControl = train_control)
lda_results <- lda_model$results
```

```{r}
library(caret)

# Calculate the correlation matrix
cor_matrix <- cor(train_valid_optimal)

# Find pairs of features with correlation higher than a threshold
highly_correlated <- findCorrelation(cor_matrix, cutoff = 0.9)

# Remove one of the correlated features from the dataset
train_valid_optimal <- train_valid_optimal[,-highly_correlated]

```


```{r}
# QDA
qda_model <- train(Severity ~ ., data = train_valid_optimal, method = "qda", trControl = train_control)
qda_results <- qda_model$results
```


```{r}
# Neural Network
nnet_model <- train(Severity ~ ., data = train_valid_optimal, method = "nnet", trControl = train_control, trace = FALSE)
nnet_results <- nnet_model$results
```


```{r}
# AdaBoost
require(fastAdaboost)
ada_boost_model <- train(Severity ~ ., data = train_valid_optimal, method = "adaboost", trControl = train_control)
ada_boost_results <- ada_boost_model$results
```


```{r}
# Random Forest
random_forest_model <- train(Severity ~ ., data = train_valid_optimal, method = "rf", trControl = train_control)
random_forest_results <- random_forest_model$results

# Logistic Regression
logistic_regression_model <- train(Severity ~ ., data = train_valid_optimal, method = "multinom", trControl = train_control, trace = FALSE)
logistic_regression_results <- logistic_regression_model$results
```


```{r}
# Decision Tree
decision_tree_model <- train(Severity ~ ., data = train_valid_optimal, method = "rpart", trControl = train_control)
decision_tree_results <- decision_tree_model$results
```


```{r}
# Extract the relevant metrics and calculate the average if there are multiple rows
rf_auc <- mean(random_forest_results$AUC)
rf_mba <- mean(random_forest_results$Mean_Balanced_Accuracy)

lr_auc <- mean(logistic_regression_results$AUC)
lr_mba <- mean(logistic_regression_results$Mean_Balanced_Accuracy)

dt_auc <- mean(decision_tree_results$AUC)
dt_mba <- mean(decision_tree_results$Mean_Balanced_Accuracy)

nnet_auc <- mean(nnet_results$AUC)
nnet_mba <- mean(nnet_results$Mean_Balanced_Accuracy)

bag_auc <- mean(bagging_results$AUC)
bag_mba <- mean(bagging_results$Mean_Balanced_Accuracy)

nb_auc <- mean(naive_bayes_results$AUC)
nb_mba <- mean(naive_bayes_results$Mean_Balanced_Accuracy)

lda_auc <- mean(lda_results$AUC)
lda_mba <- mean(lda_results$Mean_Balanced_Accuracy)

# Create a data frame with the average metrics
comparison_df <- data.frame(Model = c("Random Forest", "Logistic Regression", "Decision Tree", "Neural Network", "Bagging", "Naive Bayes", "LDA"),
                            AUC = c(rf_auc, lr_auc, dt_auc, nnet_auc, bag_auc, nb_auc, lda_auc),
                            Mean_Balanced_Accuracy = c(rf_mba, lr_mba, dt_mba, nnet_mba, bag_mba, nb_mba, lda_mba))

# Rank the models based on AUC and Mean_Balanced_Accuracy
comparison_df$AUC_Rank <- rank(-comparison_df$AUC)
comparison_df$Mean_Balanced_Accuracy_Rank <- rank(-comparison_df$Mean_Balanced_Accuracy)

# Print the comparison table
comparison_df

```


```{r}
# Load ggplot2
library(ggplot2)

# Add accuracy percentages to the comparison_df
comparison_df$Accuracy_Percentage <- c(mean(random_forest_results$Accuracy) * 100,
                                       mean(logistic_regression_results$Accuracy) * 100,
                                       mean(decision_tree_results$Accuracy) * 100,
                                       mean(nnet_results$Accuracy) * 100,
                                       mean(bagging_results$Accuracy) * 100,
                                       mean(naive_bayes_results$Accuracy) * 100,
                                       mean(lda_results$Accuracy) * 100)

# Order the comparison_df by accuracy in descending order
comparison_df <- comparison_df[order(-comparison_df$Accuracy_Percentage),]

# Create a factor with the ordered models for the x-axis
comparison_df$Model <- factor(comparison_df$Model, levels = comparison_df$Model)

# Create a bar plot comparing each model's accuracy
accuracy_plot <- ggplot(comparison_df, aes(x = Model, y = Accuracy_Percentage, fill = Model)) +
  geom_bar(stat = "identity", width = 0.7) +
  scale_fill_brewer(palette = "Set2") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(x = "Model", y = "Accuracy (%)", title = "Model Accuracy Comparison") +
  guides(fill = FALSE)

# Print the plot
accuracy_plot

```
```{r}
# Save the accuracy_plot as a PNG file
ggsave(filename = "accuracy_plot.png", plot = accuracy_plot, width = 10, height = 6, units = "in", dpi = 300)

```



